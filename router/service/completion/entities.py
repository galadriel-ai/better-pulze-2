from typing import List, Optional, Union, Dict

from pydantic import BaseModel, Field


class FunctionCall(BaseModel):
    arguments: str = Field(
        description="The name and arguments of a function that should be called, as generated by the model."
    )
    name: str = Field(description="The name of the function to call.")


class BaseMessage(BaseModel):
    content: Optional[str] = Field(
        description="The contents of the message. `content is required for all messages, and may be null for assistant messages with function calls."
    )
    function_call: Optional[FunctionCall] = Field(
        description="The name and arguments of a function that should be called, as generated by the model.",
        default=None,
    )
    role: str = Field(description="Role")


class Message(BaseMessage):
    name: Optional[str] = Field(
        description="The name of the author of this message. name is required if role is function, and it should be the name of the function whose response is in the content. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.",
        default=None,
    )


class Function(BaseModel):
    description: Optional[str] = Field(
        description="A description of what the function does, used by the model to choose when and how to call the function.",
        default=None,
    )
    name: str = Field(
        description="The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.",
    )
    parameters: Dict = Field(
        description='The parameters the functions accepts, described as a JSON Schema object. See the guide for examples, and the JSON Schema reference for documentation about the format. To describe a function that accepts no parameters, provide the value {"type": "object", "properties": {}}.'
    )


class ChatCompletionRequest(BaseModel):
    messages: List[Message] = Field(
        ..., description="A list of messages comprising the conversation so far. "
    )
    model: str = Field(description="ID of the model to use.")
    frequency_penalty: str = Field(
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
        default=None,
    )
    function_call: Union[str, Dict, None] = Field(
        description='Controls how the model calls functions. "none" means the model will not call a function and instead generates a message. "auto" means the model can pick between generating a message or calling a function. Specifying a particular function via {"name": "my_function"} forces the model to call that function. "none" is the default when no functions are present. "auto" is the default if functions are present.',
        default=None,
    )
    functions: Optional[List[Function]] = Field(
        description="A list of functions the model may generate JSON inputs for.",
        default=None,
    )
    logit_bias: Optional[Dict] = Field(
        description="Modify the likelihood of specified tokens appearing in the completion.",
        default=None,
    )
    max_tokens: Optional[int] = Field(
        description="The maximum number of tokens to generate in the chat completion.",
        default=None,
    )
    n: Optional[int] = Field(
        description="How many chat completion choices to generate for each input message.",
        default=None,
    )
    presence_penalty: Union[float, List, None] = Field(
        description="Up to 4 sequences where the API will stop generating further tokens.",
        default=0,
    )
    stream: Optional[bool] = Field(
        description="If set, partial message deltas will be sent, like in ChatGPT.",
        default=False,
    )
    temperature: Optional[float] = Field(
        description="What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
        default=1,
    )
    top_p: Optional[float] = Field(
        description="An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
        default=1,
    )
    user: Optional[str] = Field(
        description="A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.",
        default=None,
    )

    class Config:
        json_schema_extra = {
            "example": {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"},
                ],
            }
        }


class Choice(BaseModel):
    finish_reason: str = Field(
        description="The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, or function_call if the model called a function."
    )
    index: int = Field(description="The index of the choice in the list of choices.")
    message: BaseMessage = Field(
        description="A chat completion message generated by the model."
    )


class Usage(BaseModel):
    completion_tokens: int = Field(
        description="Number of tokens in the generated completion."
    )
    prompt_tokens: int = Field(description="Number of tokens in the prompt.")
    total_tokens: int = Field(
        description="Total number of tokens used in the request (prompt + completion)."
    )


class ChatCompletionResponse(BaseModel):
    id: str = Field(
        description="A unique identifier for the chat completion.",
    )
    choices: List[Choice] = Field(
        description="A list of chat completion choices. Can be more than one if `n` is greater than 1."
    )
    created: int = Field(
        description="The Unix timestamp (in seconds) of when the chat completion was created."
    )
    model: str = Field(description="The model used for the chat completion.")
    object: str = Field(
        description="The object type, which is always `chat.completion."
    )
    usage: Usage = Field(description="Usage statistics for the completion request.")

    class Config:
        json_schema_extra = {
            "example": {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1677652288,
                "model": "gpt-3.5-turbo-0613",
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": "\n\nHello there, how may I assist you today?",
                        },
                        "finish_reason": "stop",
                    }
                ],
                "usage": {
                    "prompt_tokens": 9,
                    "completion_tokens": 12,
                    "total_tokens": 21,
                },
            }
        }
